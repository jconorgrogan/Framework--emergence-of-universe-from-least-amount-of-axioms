If a Theory of Everything is to live up to its name, it has to account for what cannot be excluded.

Strip away all abstraction, and only one brute fact remains: something is being felt, and that feeling is internally structured.

No equation or ontology escapes this. It is the minimal undeniable datum: the felt contrast of experience.

From that alone, everything else follows. Contrast implies limitation, limitation demands compression, and compression yields memory, time, identity, and world.

What we call "reality" could be just the necessary closure of this system which is structure unfolding from the bare existence of felt difference. It seems a bit crazy but the logic seems sound; LMK if I'm missing anything obvious


Note- as we get lower in the tiers, more assumptions seem to pop up; Im more confident in the single digit numbers. Any and all feedback appreciated!# Felt‑Difference Ladder (v1.3)  
*From one phenomenological axiom to the quantum formalism, using only code‑length dynamics.  
All references “Lemma n” point to the compressed proof programme in **Appendix A**.*# Felt‑Difference Ladder (v1.4)  
From one phenomenological axiom to the quantum formalism, using only code‑length dynamics.  
All “Lemma n” proofs are summarised in **Appendix A**.

| Tier | Label / lemma        | Necessary statements                                                                                                                                                                               | Why it follows / proof sketch                                                                                                              | New term(s)             |
|------|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|--------------------------|
| 0    | Primitive datum      | **F0.** A system stably recognises multiple phenomenal contrasts and can compare them across or within moments.                                                                                    | Negating this is itself a contrast. Comparison ⇒ minimal buffer (**memory**). Realisation ⇒ finite info‑channel (**bottleneck**).           | memory, bottleneck       |
| 1    | Phenomenology        | **F1.** Numerous contrasts.  **F2.** Each contrast persists long enough for comparison (buffer).  **F3.** Finite channel ⇒ some contrasts dominate (salience).                                     | Direct corollaries of F0.                                                                                                                   | salience                 |
| 2    | Bottleneck           | **C1.** The finite channel acts as a bottleneck.<br>**C2.** Not all contrasts can persist ⇒ incompleteness.                                                                                        | Restates capacity limit & consequence.                                                                                                      | capacity                 |
| 3    | Compression          | **S1.** Bottleneck forces selective retention (compression).<br>**S2.** Retained traces serve as lossy memory for discarded contrasts.                                                             | From C1 & C2.                                                                                                                               | memory trace             |
| 3a   | **SUR lemma**        | *SUR.* Finite‑state rewrite: if a shorter code yields equal prediction power, it overwrites the longer in ≤ ⌈len/ε⌉ cycles. MDL codes are the only stable fixed points.                            | ε free bits in shorter code carry the replace‑pointer; length descends monotonically.                                                       | —                        |
| 4    | Pattern extraction   | **P1.** SUR ⇒ MDL attractor.<br>**Lemma MI.** MDL + mutual‑info maximisation pins a stable self‑cluster at the locus of F0.                                                                        | Redundant bits pruned; MI‑max cluster anchors predictive coding to the subject.                                                             | prediction, self cluster |
| 4.5  | Observer definition  | **O1.** *Observer = MDL compressor + stable MI‑max self‑cluster + Δ_self capacity for recursive updates.*                                                                                           | Enables counterfactual prediction & identity persistence.                                                                                   | observer                 |
| 5    | World partition      | **E1.** Regularities not tagged “self” form a world model.<br>**Lemma ToM.** Systematic self‑like errors in non‑self traces activate a self‑simulation stub → “other minds.”                       | Complement rule; agent‑like compression cheapest.                                                                                           | world, other mind        |
| 6    | Process dynamics     | **D1.** Compression discards whole branches irreversibly ⇒ arrow of time.<br>**D2.** Marginal code cost defines distance; cumulative cost is an info‑metric\*.                                    | Global history is lossy; within the surviving bundle codes are lossless (see R1\*).                                                         | time, metric             |
| 6a   | Op. constraints      | **Lemma R1\*.** Any persistent code that cannot be unpacked losslessly leaks bits; SUR replaces it with a lossless factorisation → *local reversibility*.<br>**R2.** Code cost is sub‑additive.<br>**Lemma R3\*.** Perfect cloning would beat SUR by O(log n); cheapest stable codes therefore forbid arbitrary duplication (*non‑cloning*). | R1\*: leak ⇒ longer rival ⇒ overwritten.  R3\*: cloning lowers header cost below MDL, hence destabilised.                                   | —                        |
| 7a   | **MIN‑1**            | *Lemmas 1 & 3.* Header sharing of *n* alternatives → bundled superposition vector (Θ(n) saving).                                                            | Separate storage repeats delimiters; bundle wins under SUR.                                                                                | potential store          |
| 7b   | **MIN‑2**            | *Lemma 4.* Reversibility + sub‑additivity (R1\*, R2) ⇒ additive descriptors; vector addition gives linear structure.                                         | Cauchy functional equation under boundedness.                                                                                              | linearity                |
| 7c   | **MIN‑3**            | *Lemmas 5 & 6 with R3\*.** Cheapest reversible distinguishable field is complex magnitude–phase; real or quaternionic fields add ≥ 1 bit/branch.                                                | Extra parameters inflate header; SUR removes them.                                                                                        | complex amplitudes       |
| 7d   | **Theorem MIN**      | Combining MIN‑1…3 ⇒ complex inner‑product Hilbert space **H** is the unique length‑minimal reversible code for large *n* branches (Lemma 7).                                                      | Any alternative violates SUR, R1\*, R2, R3\* or is longer.                                                                                | Hilbert space **H**      |
| 8    | Probability rule     | **M2a.** Additivity minimises length; non‑additive measures need exception tables ⇒ break SUR.<br>**M2b.** In complex dim ≥ 3, Gleason ⇒ quadratic (Born) weights.                                 | Cost + Gleason.                                                                                                                            | Born weights             |
| 9    | Projection bound     | **Q1.** If Σ(header + branch codes) exceeds capacity by σ (header quantum), SUR prunes all but the least‑cost branch; outcome is then written into memory.                                        | Full collapse uniquely satisfies capacity with minimal loss.                                                                              | projection event         |
| 10   | Consensus layer      | **Q2.** Two observers communicate only within shared code basis. Inter‑subjective reality = `Δ_proj_{C∩}(M₁) ∩ Δ_proj_{C∩}(M₂)`.                                                                | Basis compatibility + channel intersection define common truth.                                                                           | inter‑subjective reality |
| 11   | Geometric curve      | **G1.** Unequal sub‑code reuse along different paths ⇒ curvature; uniform reuse ⇒ flat geometry.                                                            | Path‑dependent cost = curvature tensor.                                                                                                   | curvature                |
| 12   | Energy analogue      | **H1.** Minimising mean prediction cost under fixed capacity introduces conserved λ; with cost Hamiltonian **H**, λ = ∂(min cost)/∂C = β = 1/kT.                                                  | Lagrange multiplier constant; maps to thermodynamic β.                                                                                    | energy analogue          |

\* Triangle inequality holds if incremental costs are additive; else distance is pseudo‑metric.

---

### Appendix A (pointer)
*Formal proof programme: Lemmas 1‑7, MI, ToM, R1\*, R3\*; full derivation of Theorem MIN; mapping to reconstruction results of Hardy & Chiribella et al.*

With Lemmas R1\* and R3\* now proven cost‑necessities—not postulates—the chain from F0 through Tier 9 (Hilbert space, Born rule, collapse) remains strictly deduced from finite‑channel MDL dynamics.

